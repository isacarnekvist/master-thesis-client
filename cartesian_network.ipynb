{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import threading\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "from multiprocessing import Process, Queue, Value, Pool\n",
    "if sys.version_info.major == 2:\n",
    "    from Queue import Empty\n",
    "else:\n",
    "    from queue import Empty\n",
    "\n",
    "import keras\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Merge, Input, Lambda, merge, Layer, BatchNormalization\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#from ddpg import Actor, Critic\n",
    "from naf.priority_buffer import PriorityBuffer\n",
    "from environment import Environment, WIN, LOSE, NEUTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "e = Environment(0.01)\n",
    "replay_buffer = PriorityBuffer(2 ** 20)\n",
    "\n",
    "rewards = []\n",
    "for trials in range(4096):\n",
    "    e.reset()\n",
    "    for i in range(32):\n",
    "        mu = e.heuristic_move()\n",
    "        x1 = e.get_state()\n",
    "        s, r, x2 = e.interact(*mu)\n",
    "        replay_buffer.add({\n",
    "            'x1': x1,\n",
    "            'x2': x2,\n",
    "            'u': mu,\n",
    "            'r': r\n",
    "        }).set_value(10.0)\n",
    "        if s == WIN:\n",
    "            break\n",
    "        \n",
    "replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import mul\n",
    "from functools import reduce\n",
    "\n",
    "import keras\n",
    "import theano\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Merge, Input, Lambda, BatchNormalization\n",
    "\n",
    "\n",
    "class DDPG:\n",
    "\n",
    "    def predict(self, *x):\n",
    "        return self.nn.predict(list(x))\n",
    "    \n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return [w for w in self.nn.trainable_weights if not w.name.startswith('bn')]\n",
    "    \n",
    "    def soft_update(self, weights, lr=0.001):\n",
    "        \"\"\"\n",
    "        Accepts theano tensors as inputs\n",
    "        \"\"\"\n",
    "        for w_old, w_new in zip(self.nn.weights, weights):\n",
    "            w_old.set_value(\n",
    "                lr * w_new.get_value() + (1 - lr) * w_old.get_value()\n",
    "            )\n",
    "            \n",
    "\n",
    "class Critic(DDPG):\n",
    "    \n",
    "    def __init__(self, x_size, u_size, hidden_size=100):\n",
    "    \n",
    "        super(Critic, self).__init__()\n",
    "        x = Input(shape=(x_size, ), name='x')\n",
    "        u = Input(shape=(u_size, ), name='u')\n",
    "        x_model = Model(input=x, output=x)\n",
    "        u_model = Model(input=u, output=u)\n",
    "        \n",
    "        first_part = Sequential([\n",
    "            BatchNormalization(input_shape=(x_size,), name='bn1'),\n",
    "            Dense(output_dim=hidden_size, activation='relu', name='fc1', W_regularizer=l2(0.01)),\n",
    "        ])\n",
    "        \n",
    "        self.nn = Sequential([\n",
    "            Merge([first_part, u_model], mode='concat'),\n",
    "            BatchNormalization(name='bn2'),\n",
    "            Dense(output_dim=hidden_size - 100, activation='relu', name='fc2', W_regularizer=l2(0.01)),\n",
    "            BatchNormalization(name='bn3'),\n",
    "            Dense(output_dim=(1), name='Q', W_regularizer=l2(0.01)),\n",
    "        ])\n",
    "\n",
    "        adam = Adam(lr=0.0001)\n",
    "        self.nn.compile(loss='mse', optimizer=adam)\n",
    "        self._gradients = theano.function(\n",
    "            self.nn.inputs + [K.learning_phase()],\n",
    "            T.grad(self.nn.output[0, 0], u_model.output),\n",
    "            allow_input_downcast=True\n",
    "        )\n",
    "\n",
    "    def gradients(self, x, u):\n",
    "        assert x.shape[0] == 1\n",
    "        return self._gradients(x, u, False)\n",
    "        \n",
    "    \n",
    "class Actor(DDPG):\n",
    "    \n",
    "    def __init__(self, x_size, u_size, mu_scaling, hidden_size=100):\n",
    "        \n",
    "        # for Adam\n",
    "        self.t = 0\n",
    "        self.alpha = 0.001\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.epsilon = 1e-8\n",
    "\n",
    "        super(Actor, self).__init__()\n",
    "        x = Input(shape=(x_size, ), name='state')\n",
    "        self.nn = Sequential([\n",
    "            BatchNormalization(input_shape=(x_size,), name='bn1'),\n",
    "            Dense(input_shape=(x_size,), output_dim=hidden_size, activation='relu', name='fc1'),\n",
    "            BatchNormalization(name='bn2'),\n",
    "            Dense(output_dim=hidden_size - 100, activation='relu', name='fc2'),\n",
    "            BatchNormalization(name='bn3'),\n",
    "            Dense(output_dim=u_size, name='mu_unscaled', activation='tanh'),\n",
    "            Lambda(lambda x: mu_scaling * x, output_shape=(u_size, ), name='mu')\n",
    "        ])\n",
    "        \n",
    "        # This optimizer won't be needed, learning from policy gradient\n",
    "        self.nn.compile(loss='mse', optimizer='sgd')\n",
    "        \n",
    "        # gradients\n",
    "        params = self.trainable_weights\n",
    "        gradients = [T.grad(self.nn.output[0, i], params) for i in range(u_size)]\n",
    "        gradients_list = []\n",
    "        for g in gradients:\n",
    "            gradients_list.extend(g)\n",
    "        self._gradients = theano.function(\n",
    "            self.nn.inputs + [K.learning_phase()],\n",
    "            gradients_list,\n",
    "            allow_input_downcast=True\n",
    "        )\n",
    "    \n",
    "    def gradients(self, x):\n",
    "        assert x.shape[0] == 1\n",
    "        res = []\n",
    "        for g in self._gradients(x, False):\n",
    "            res.extend(g.flatten())\n",
    "        return np.array(res).reshape((2, int(len(res) / 2)))\n",
    "    \n",
    "    def update_with_policy_gradient(self, policy_gradient):\n",
    "        \"\"\"\n",
    "        Update from separate actor and critic gradients, which\n",
    "        multiply to make the policy gradient\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        if self.t == 0:\n",
    "            self.m = np.zeros(policy_gradient.shape)\n",
    "            self.v = np.zeros(policy_gradient.shape)\n",
    "        self.t += 1\n",
    "        self.m = self.beta1 * self.m + (1 - self.beta1) * policy_gradient\n",
    "        self.v = self.beta2 * self.v + (1 - self.beta2) * policy_gradient ** 2\n",
    "        m_hat = self.m / (1 - self.beta1 ** self.t)\n",
    "        v_hat = self.v / (1 - self.beta2 ** self.t)\n",
    "        policy_gradient = policy_gradient.astype(np.float32)\n",
    "        for g in self.trainable_weights:\n",
    "            prev = g.get_value()\n",
    "            param_len = reduce(mul, prev.shape)\n",
    "            mh = m_hat[0, i:i + param_len].reshape(prev.shape).astype(np.float32)\n",
    "            vh = v_hat[0, i:i + param_len].reshape(prev.shape).astype(np.float32)\n",
    "            g.set_value(prev + self.alpha * mh / (np.sqrt(vh) + self.epsilon))\n",
    "            i += param_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_DIST = 0.01\n",
    "\n",
    "hidden_size = 400\n",
    "actor = Actor((2 + 2 + 2), 2, MAX_DIST, hidden_size=hidden_size)\n",
    "#actor_target = Actor((2 + 2 + 2), 2, MAX_DIST, hidden_size=hidden_size)\n",
    "#actor_target.nn.set_weights(actor.nn.get_weights())\n",
    "\n",
    "critic = Critic((2 + 2 + 2), 2, hidden_size=hidden_size)\n",
    "#critic_target = Critic((2 + 2 + 2), 2, hidden_size=hidden_size)\n",
    "#critic_target.nn.set_weights(critic.nn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.random.randn(1, 6)\n",
    "U = np.random.randn(1, 2)\n",
    "actor.update_with_policy_gradient(np.dot(critic.gradients(X, U), actor.gradients(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#def plot_v(nn, cube_x, cube_y, goal_x, goal_y):\n",
    "#    xs = np.linspace(-0.15, 0.15, 12)\n",
    "#    ys = np.linspace(0.10, 0.30, 12)\n",
    "#    xss, yss = np.meshgrid(xs, ys)\n",
    "#    zss = np.zeros(xss.shape)\n",
    "#    for i, x in enumerate(xs):\n",
    "#        for j, y in enumerate(ys):\n",
    "#            zss[len(ys) - j - 1, i] = nn.v.predict(np.array([[x, y, cube_x, cube_y, goal_x, goal_y]]))[0, 0]\n",
    "#    plt.imshow(zss, cmap='inferno', interpolation='gaussian', aspect='auto',\n",
    "#               extent=[-0.15, 0.15, 0.10, 0.30])\n",
    "#    plt.plot(cube_x, cube_y, 'ko', markersize=8)\n",
    "#    plt.plot(cube_x, cube_y, 'ro', markersize=6)\n",
    "#    plt.plot(goal_x, goal_y, 'ko', markersize=8)\n",
    "#    plt.plot(goal_x, goal_y, 'wo', markersize=6)\n",
    "#    plt.colorbar().set_label('$V(\\mathbf{x})$')\n",
    "    \n",
    "def plot_q(nn, eef_x, eef_y, cube_x, cube_y, goal_x, goal_y):\n",
    "    xs = np.linspace(-0.01, 0.01, 12)\n",
    "    ys = np.linspace(-0.01, 0.01, 12)\n",
    "    xss, yss = np.meshgrid(xs, ys)\n",
    "    zss = np.zeros(xss.shape)\n",
    "    for i, x in enumerate(xs):\n",
    "        for j, y in enumerate(ys):\n",
    "            zss[len(ys) - j - 1, i] = nn.predict(\n",
    "                np.array([[eef_x, eef_y, cube_x, cube_y, goal_x, goal_y]]),\n",
    "                np.array([[x, y]])\n",
    "            )[0, 0]\n",
    "    plt.imshow(zss, cmap='inferno', interpolation='gaussian', aspect='auto',\n",
    "               extent=[-0.01, 0.01, -0.01, 0.01])\n",
    "    plt.plot(0.0, 0.0, 'ko', markersize=10)\n",
    "    plt.plot(0.0, 0.0, 'w+', markersize=10)\n",
    "    plt.xticks(np.linspace(-0.01, 0.01, 5))\n",
    "    plt.yticks(np.linspace(0.01, -0.01, 5))\n",
    "    plt.colorbar().set_label('$Q(\\mathbf{x, u})$')\n",
    "\n",
    "\n",
    "def plot_pi(nn, cube_x, cube_y, goal_x, goal_y, eef=None):\n",
    "    for x in np.linspace(-0.15, 0.15, 20):\n",
    "        for y in np.linspace(0.12, 0.30, 20):\n",
    "            X = np.array([[x, y, cube_x, cube_y, goal_x, goal_y]])\n",
    "            dx, dy = nn.predict(X)[0, :]\n",
    "            plt.arrow(x, y, dx, dy)\n",
    "    if eef:\n",
    "        plt.plot(eef[0], eef[1], 'ko', markersize=10)\n",
    "        plt.plot(eef[0], eef[1], 'w+', markersize=10)\n",
    "    plt.plot(cube_x, cube_y, 'ko', markersize=10)\n",
    "    plt.plot(cube_x, cube_y, 'ro', markersize=8)\n",
    "    plt.plot(goal_x, goal_y, 'ko', markersize=10)\n",
    "    plt.plot(goal_x, goal_y, 'wo', markersize=8)\n",
    "    plt.title('$\\mathbf{\\mu(x)}$')\n",
    "    plt.xlim(-0.15, 0.15)\n",
    "    plt.ylim(0.12, 0.30)\n",
    "    print('dx, dy:', dx, dy)\n",
    "    \n",
    "#e.reset()\n",
    "#plt.figure(figsize=(13, 4))\n",
    "#plt.subplot(121)\n",
    "#plot_pi(actor, e.circle.x, e.circle.y, e.goal_x, e.goal_y, eef=(e.eef_x, e.eef_y))\n",
    "#plt.subplot(122)\n",
    "#plot_q(critic, e.eef_x, e.eef_y, e.circle.x, e.circle.y, e.goal_x, e.goal_y)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def return_average(actor, gamma=0.98):\n",
    "    rewards = []\n",
    "    for trial in range(8):\n",
    "        np.random.seed(trial)\n",
    "        e.reset()\n",
    "        n_steps = 256\n",
    "        return_ = 0.0\n",
    "        for i in range(n_steps):\n",
    "            mu = actor.predict(e.get_state())\n",
    "            _, r, _ = e.interact(*mu.flatten())\n",
    "            return_ += gamma ** i * r\n",
    "        rewards.append(return_)\n",
    "    return np.mean(rewards), np.std(rewards)\n",
    "\n",
    "#return_average(actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "epoch_size = 256\n",
    "batch_size = 32\n",
    "gamma = 0.98\n",
    "epsilon = 0.1\n",
    "\n",
    "#reward_averages = []\n",
    "\n",
    "X = np.zeros((epoch_size, 6))\n",
    "Xp = np.zeros((epoch_size, 6))\n",
    "U = np.zeros((epoch_size, 2))\n",
    "R = np.zeros((epoch_size, 1))\n",
    "gradient_len = actor.gradients(X[:1, :]).shape[1]\n",
    "policy_gradient = np.zeros((1, gradient_len))\n",
    "\n",
    "n_iterations = 2048.0\n",
    "latest_plot = datetime.now() - timedelta(seconds=30)\n",
    "latest_trial_plot = datetime.now() - timedelta(seconds=60)\n",
    "#a = 0\n",
    "for a in range(a, int(n_iterations)):\n",
    "    print('iteration {} / {}'.format(a + 1, n_iterations))\n",
    "    #e.reset()\n",
    "    #latest_trial = []\n",
    "    #latest_rewards = []\n",
    "    #for b in range(batch_size):\n",
    "    #    x1 = e.get_state()\n",
    "    #    mu = actor.predict(x1, epsilon=1.0 * (n_iterations - a) / n_iterations + 0.1 * a / n_iterations)\n",
    "    #        \n",
    "    #    state, reward, x2 = e.interact(*(mu)[0, :])\n",
    "    #    latest_trial.append(x2[0, :])\n",
    "    #    latest_rewards.append(reward)\n",
    "    #    replay_buffer.add({\n",
    "    #        'x1': x1,\n",
    "    #        'x2': x2,\n",
    "    #        'u': mu,\n",
    "    #        'r': reward\n",
    "    #    }).set_value(10.0)\n",
    "    #    if state in [LOSE, WIN] or b == batch_size - 1 or len(latest_trial) % 32 == 0:\n",
    "    #        latest_trial = []\n",
    "    #        latest_rewards = []\n",
    "    #        e.reset()\n",
    "    \n",
    "    n_inner = 4\n",
    "    for i in range(n_inner):\n",
    "        exp_nodes = []\n",
    "        for b in range(epoch_size):\n",
    "            sample = replay_buffer.sample()\n",
    "            exp_nodes.append(sample)\n",
    "            X[b, :] = sample.data['x1']\n",
    "            Xp[b, :] = sample.data['x2']\n",
    "            R[b, :] = sample.data['r']\n",
    "            U[b, :] = sample.data['u']\n",
    "            \n",
    "        timestamp = datetime.now()\n",
    "        Q = critic.predict(X, U)\n",
    "        Y = R + gamma * critic_target.predict(Xp, actor_target.predict(Xp))\n",
    "        [node.set_value(abs(delta) + epsilon) for node, delta in zip(exp_nodes, (Q - Y)[:, 0])]\n",
    "        beta = np.exp((a - n_iterations) / (0.1 * n_iterations))\n",
    "        sample_weight = np.array([1.0 / node.value for node in exp_nodes]) ** beta\n",
    "        print('batch prepared, took {}'.format(datetime.now() - timestamp))\n",
    "\n",
    "        timestamp = datetime.now()\n",
    "        critic.nn.fit([X, U], Y, verbose=0, sample_weight=sample_weight, batch_size=batch_size, nb_epoch=1)\n",
    "        print('fit() took {}'.format(datetime.now() - timestamp))\n",
    "        \n",
    "        timestamp = datetime.now()\n",
    "        policy_gradient *= 0\n",
    "        for b in range(epoch_size):\n",
    "            ag = actor.gradients(X[b:b + 1, :])\n",
    "            ac = critic.gradients(X[b:b + 1, :], U[b:b + 1, :])\n",
    "            policy_gradient += sample_weight[b] * np.dot(ac, ag)\n",
    "        print('gradients calculated: {}'.format(datetime.now() - timestamp))\n",
    "        \n",
    "        actor.update_with_policy_gradient(policy_gradient)\n",
    "        actor_target.soft_update(actor.nn.weights, lr=0.001)\n",
    "        critic_target.soft_update(critic.nn.weights, lr=0.001)\n",
    "\n",
    "        if datetime.now() > latest_plot + timedelta(seconds=15):\n",
    "            print('beta: {} outer: {}/{} inner: {}/{} {}'.format(beta, a, n_iterations, i, n_inner, replay_buffer))\n",
    "            now = datetime.now()\n",
    "            np.random.seed(now.microsecond + now.second + now.minute)\n",
    "            e.reset()\n",
    "            plt.figure(figsize=(13, 8))\n",
    "            plt.subplot(221)\n",
    "            plot_pi(actor, e.circle.x, e.circle.y, e.goal_x, e.goal_y, eef=(e.eef_x, e.eef_y))\n",
    "            plt.subplot(222)\n",
    "            plot_q(critic, e.eef_x, e.eef_y, e.circle.x, e.circle.y, e.goal_x, e.goal_y)\n",
    "            plt.subplot(223)\n",
    "            plot_pi(actor_target, e.circle.x, e.circle.y, e.goal_x, e.goal_y, eef=(e.eef_x, e.eef_y))\n",
    "            plt.subplot(224)\n",
    "            plot_q(critic_target, e.eef_x, e.eef_y, e.circle.x, e.circle.y, e.goal_x, e.goal_y)\n",
    "            plt.show()\n",
    "            \n",
    "            r_avg, r_std = return_average(actor_target)\n",
    "            reward_averages.append(r_avg)\n",
    "            plt.title('Average test trial return')\n",
    "            plt.plot(reward_averages)\n",
    "            plt.show()\n",
    "            \n",
    "            latest_plot = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datetime.now().second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e.reset()\n",
    "e.eef_x = 0.05\n",
    "e.eef_y = 0.20\n",
    "e.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
