{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import threading\n",
    "from operator import mul\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "if sys.version_info.major == 2:\n",
    "    from Queue import Queue\n",
    "else:\n",
    "    from queue import Queue\n",
    "\n",
    "import keras\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Merge, Input, Lambda, merge, Layer, BatchNormalization\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from naf.priority_buffer import PriorityBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    \n",
    "    def predict(self, *x):\n",
    "        return self.nn.predict(list(x))\n",
    "    \n",
    "    @property\n",
    "    def trainable_weights(self):\n",
    "        return [w for w in self.nn.trainable_weights if not w.name.startswith('bn')]\n",
    "    \n",
    "    def soft_update(self, weights, lr=0.001):\n",
    "        \"\"\"\n",
    "        Accepts theano tensors as inputs\n",
    "        \"\"\"\n",
    "        for w_old, w_new in zip(self.nn.weights, weights):\n",
    "            w_old.set_value(\n",
    "                lr * w_new.get_value() + (1 - lr) * w_old.get_value()\n",
    "            )\n",
    "            \n",
    "\n",
    "class Critic(DDPG):\n",
    "    \n",
    "    def __init__(self, x_size, u_size, hidden_size=100):\n",
    "    \n",
    "        x = Input(shape=(x_size, ), name='x')\n",
    "        u = Input(shape=(u_size, ), name='u')\n",
    "        x_model = Model(input=x, output=x)\n",
    "        u_model = Model(input=u, output=u)\n",
    "        \n",
    "        self.nn = Sequential([\n",
    "            Merge([x_model, u_model], mode='concat'),\n",
    "            BatchNormalization(input_shape=(x_size,), name='bn1'),\n",
    "            Dense(output_dim=hidden_size, activation='relu', name='fc1'),\n",
    "            BatchNormalization(name='bn2'),\n",
    "            Dense(output_dim=hidden_size, activation='relu', name='fc2'),\n",
    "            BatchNormalization(name='bn3'),\n",
    "            Dense(output_dim=(1), name='Q'),\n",
    "        ])\n",
    "\n",
    "        adam = Adam(lr=0.0001)\n",
    "        self.nn.compile(loss='mse', optimizer=adam)\n",
    "        self._gradients = theano.function(\n",
    "            self.nn.inputs + [K.learning_phase()],\n",
    "            T.grad(self.nn.output[0, 0], u_model.output),\n",
    "            allow_input_downcast=True\n",
    "        )\n",
    "\n",
    "    def gradients(self, x, u):\n",
    "        assert x.shape[0] == 1\n",
    "        return self._gradients(x, u, False)\n",
    "        \n",
    "    \n",
    "class Actor(DDPG):\n",
    "    \n",
    "    def __init__(self, x_size, u_size, mu_scaling, hidden_size=100):\n",
    "        x = Input(shape=(x_size, ), name='state')\n",
    "        self.nn = Sequential([\n",
    "            BatchNormalization(input_shape=(x_size,), name='bn1'),\n",
    "            Dense(input_shape=(x_size,), output_dim=hidden_size, activation='relu', name='fc1'),\n",
    "            BatchNormalization(name='bn2'),\n",
    "            Dense(output_dim=hidden_size, activation='relu', name='fc2'),\n",
    "            BatchNormalization(name='bn3'),\n",
    "            Dense(output_dim=u_size, name='mu_unscaled', activation='tanh'),\n",
    "            Lambda(lambda x: mu_scaling * x, output_shape=(u_size, ), name='mu')\n",
    "        ])\n",
    "        \n",
    "        # This optimizer won't be needed, learning from policy gradient\n",
    "        self.nn.compile(loss='mse', optimizer='sgd')\n",
    "        \n",
    "        # gradients\n",
    "        params = self.trainable_weights\n",
    "        gradients = [T.grad(self.nn.output[0, i], params) for i in range(u_size)]\n",
    "        gradients_list = []\n",
    "        for g in gradients:\n",
    "            gradients_list.extend(g)\n",
    "        self._gradients = theano.function(\n",
    "            self.nn.inputs + [K.learning_phase()],\n",
    "            gradients_list,\n",
    "            allow_input_downcast=True\n",
    "        )\n",
    "    \n",
    "    def gradients(self, x):\n",
    "        assert x.shape[0] == 1\n",
    "        res = []\n",
    "        for g in self._gradients(x, False):\n",
    "            res.extend(g.flatten())\n",
    "        return np.array(res).reshape((2, int(len(res) / 2)))\n",
    "    \n",
    "    def update_with_policy_gradient(self, policy_gradient, lr=0.0001):\n",
    "        \"\"\"\n",
    "        Update from separate actor and critic gradients, which\n",
    "        multiply to make the policy gradient\n",
    "        \"\"\"\n",
    "        i = 0\n",
    "        policy_gradient = policy_gradient.astype(np.float32)\n",
    "        for g in self.trainable_weights:\n",
    "            v = g.get_value()\n",
    "            param_len = reduce(mul, v.shape)\n",
    "            g.set_value(v + lr * policy_gradient[0, i:i + param_len].reshape(v.shape))\n",
    "            i += param_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_state_vector(eef_x, eef_y, circle_x, circle_y, goal_x, goal_y):\n",
    "    return np.array([\n",
    "        [eef_x, eef_y, circle_x, circle_y, goal_x, goal_y]\n",
    "    ], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WIN = 0\n",
    "LOSE = 1\n",
    "NEUTRAL = 2\n",
    "MAX_DIST = 0.01\n",
    "\n",
    "class Circle:\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.radius = 0.02\n",
    "        \n",
    "    def interact(self, x, y):\n",
    "        theta = np.arctan2(y - self.y, x - self.x)\n",
    "        center_distance = np.linalg.norm([self.y - y, self.x - x])\n",
    "        distance = self.radius - center_distance\n",
    "        if center_distance > self.radius:\n",
    "            return\n",
    "        self.x -= distance * np.cos(theta)\n",
    "        self.y -= distance * np.sin(theta)\n",
    "        \n",
    "class Environment:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        # Random on inner and outer circle\n",
    "        eef_theta = np.random.rand() * 2 * np.pi\n",
    "        self.eef_x = 0.10 * np.cos(eef_theta)\n",
    "        self.eef_y = 0.20 + 0.07 * np.sin(eef_theta)\n",
    "        circle_theta = np.random.rand() * 2 * np.pi\n",
    "        circle_x = 0.04 * np.cos(circle_theta)\n",
    "        circle_y = 0.20 + 0.02 * np.sin(circle_theta)\n",
    "        self.circle = Circle(circle_x, circle_y)\n",
    "        while True:\n",
    "            goal_theta = np.random.rand() * 2 * np.pi\n",
    "            self.goal_x = 0.04 * np.cos(goal_theta)\n",
    "            self.goal_y = 0.20 + 0.02 * np.sin(goal_theta)\n",
    "            if np.linalg.norm([self.goal_x - circle_x, self.goal_y - circle_y]) > 0.04:\n",
    "                break\n",
    "        while True:\n",
    "            self.eef_x  = -0.10 + np.random.rand() * 0.20\n",
    "            self.eef_y  =  0.12 + np.random.rand() * 0.17\n",
    "            if np.linalg.norm([self.eef_x - circle_x, self.eef_y - circle_y]) < 0.04:\n",
    "                continue\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def get_state(self):\n",
    "        return create_state_vector(\n",
    "            self.eef_x,\n",
    "            self.eef_y,\n",
    "            self.circle.x,\n",
    "            self.circle.y,\n",
    "            self.goal_x,\n",
    "            self.goal_y,\n",
    "        )\n",
    "\n",
    "    def interact(self, dx, dy):\n",
    "        dist = np.linalg.norm([dx, dy])\n",
    "        if dist > MAX_DIST:\n",
    "            dx = MAX_DIST * dx / dist\n",
    "            dy = MAX_DIST * dy / dist\n",
    "        self.eef_x += dx\n",
    "        self.eef_y += dy\n",
    "        self.circle.interact(self.eef_x, self.eef_y)\n",
    "        state = NEUTRAL\n",
    "        reward = -4\n",
    "        if not -0.15 <= self.eef_x <= 0.15:\n",
    "            state = LOSE\n",
    "        elif not 0.10 <= self.eef_y <= 0.30:\n",
    "            state = LOSE\n",
    "        elif not -0.15 <= self.circle.x <= 0.15:\n",
    "            state = LOSE\n",
    "        elif not 0.10 <= self.circle.y <= 0.30:\n",
    "            state = LOSE\n",
    "        elif np.linalg.norm([self.goal_x - self.circle.x, self.goal_y - self.circle.y]) < 0.005:\n",
    "            state = WIN\n",
    "            \n",
    "        if state != LOSE:\n",
    "            eef2circle = np.linalg.norm([self.eef_x - self.circle.x, self.eef_y - self.circle.y])\n",
    "            circle2goal = np.linalg.norm([self.goal_x - self.circle.x, self.goal_y - self.circle.y])\n",
    "            reward = (\n",
    "                np.exp(-200 * eef2circle ** 2) - 1 +\n",
    "                2 * np.exp(-200 * circle2goal ** 2) - 1\n",
    "            )\n",
    "        \n",
    "        return state, reward, self.get_state()\n",
    "        \n",
    "    def plot(self, ax=None):\n",
    "        if ax is None:\n",
    "            _, ax = plt.subplots()\n",
    "        plt.grid()\n",
    "        ax.add_artist(plt.Circle(\n",
    "            (self.goal_x, self.goal_y),\n",
    "            self.circle.radius,\n",
    "            color='k',\n",
    "        ))\n",
    "        ax.add_artist(plt.Circle(\n",
    "            (self.goal_x, self.goal_y),\n",
    "            self.circle.radius - 0.001,\n",
    "            color='w',\n",
    "        ))\n",
    "        ax.add_artist(plt.Circle(\n",
    "            (self.circle.x, self.circle.y),\n",
    "            self.circle.radius,\n",
    "            color='r',\n",
    "            alpha=0.5\n",
    "        ))\n",
    "        plt.plot(self.eef_x, self.eef_y, 'k+', markersize=10)\n",
    "        plt.xlim((-0.15, 0.15))\n",
    "        plt.ylim((0.10, 0.30))\n",
    "        \n",
    "e = Environment()\n",
    "e.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 400\n",
    "actor = Actor((2 + 2 + 2), 2, MAX_DIST, hidden_size=hidden_size)\n",
    "actor_target = Actor((2 + 2 + 2), 2, MAX_DIST, hidden_size=hidden_size)\n",
    "actor_target.nn.set_weights(actor.nn.get_weights())\n",
    "\n",
    "critic = Critic((2 + 2 + 2), 2, hidden_size=hidden_size)\n",
    "critic_target = Critic((2 + 2 + 2), 2, hidden_size=hidden_size)\n",
    "critic_target.nn.set_weights(critic.nn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "critic.nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def plot_v(nn, cube_x, cube_y, goal_x, goal_y):\n",
    "#    xs = np.linspace(-0.15, 0.15, 12)\n",
    "#    ys = np.linspace(0.10, 0.30, 12)\n",
    "#    xss, yss = np.meshgrid(xs, ys)\n",
    "#    zss = np.zeros(xss.shape)\n",
    "#    for i, x in enumerate(xs):\n",
    "#        for j, y in enumerate(ys):\n",
    "#            zss[len(ys) - j - 1, i] = nn.v.predict(np.array([[x, y, cube_x, cube_y, goal_x, goal_y]]))[0, 0]\n",
    "#    plt.imshow(zss, cmap='inferno', interpolation='gaussian', aspect='auto',\n",
    "#               extent=[-0.15, 0.15, 0.10, 0.30])\n",
    "#    plt.plot(cube_x, cube_y, 'ko', markersize=8)\n",
    "#    plt.plot(cube_x, cube_y, 'ro', markersize=6)\n",
    "#    plt.plot(goal_x, goal_y, 'ko', markersize=8)\n",
    "#    plt.plot(goal_x, goal_y, 'wo', markersize=6)\n",
    "#    plt.colorbar().set_label('$V(\\mathbf{x})$')\n",
    "    \n",
    "def plot_q(nn, eef_x, eef_y, cube_x, cube_y, goal_x, goal_y):\n",
    "    xs = np.linspace(-0.01, 0.01, 12)\n",
    "    ys = np.linspace(-0.01, 0.01, 12)\n",
    "    xss, yss = np.meshgrid(xs, ys)\n",
    "    zss = np.zeros(xss.shape)\n",
    "    for i, x in enumerate(xs):\n",
    "        for j, y in enumerate(ys):\n",
    "            zss[len(ys) - j - 1, i] = nn.predict(\n",
    "                np.array([[eef_x, eef_y, cube_x, cube_y, goal_x, goal_y]]),\n",
    "                np.array([[x, y]])\n",
    "            )[0, 0]\n",
    "    plt.imshow(zss, cmap='inferno', interpolation='gaussian', aspect='auto',\n",
    "               extent=[-0.01, 0.01, -0.01, 0.01])\n",
    "    plt.plot(0.0, 0.0, 'ko', markersize=10)\n",
    "    plt.plot(0.0, 0.0, 'w+', markersize=10)\n",
    "    plt.xticks(np.linspace(-0.01, 0.01, 5))\n",
    "    plt.yticks(np.linspace(0.01, -0.01, 5))\n",
    "    plt.colorbar().set_label('$Q(\\mathbf{x, u})$')\n",
    "\n",
    "\n",
    "def plot_pi(nn, cube_x, cube_y, goal_x, goal_y, eef=None):\n",
    "    for x in np.linspace(-0.15, 0.15, 20):\n",
    "        for y in np.linspace(0.12, 0.30, 20):\n",
    "            X = np.array([[x, y, cube_x, cube_y, goal_x, goal_y]])\n",
    "            dx, dy = nn.predict(X)[0, :]\n",
    "            plt.arrow(x, y, 2 * dx, 2 * dy)\n",
    "    if eef:\n",
    "        plt.plot(eef[0], eef[1], 'ko', markersize=10)\n",
    "        plt.plot(eef[0], eef[1], 'w+', markersize=10)\n",
    "    plt.plot(cube_x, cube_y, 'ko', markersize=10)\n",
    "    plt.plot(cube_x, cube_y, 'ro', markersize=8)\n",
    "    plt.plot(goal_x, goal_y, 'ko', markersize=10)\n",
    "    plt.plot(goal_x, goal_y, 'wo', markersize=8)\n",
    "    plt.title('$\\mathbf{\\mu(x)}$')\n",
    "    plt.xlim(-0.15, 0.15)\n",
    "    plt.ylim(0.12, 0.30)\n",
    "    print('dx, dy:', dx, dy)\n",
    "    \n",
    "e.reset()\n",
    "plt.figure(figsize=(13, 3))\n",
    "plt.subplot(121)\n",
    "plot_pi(actor, e.circle.x, e.circle.y, e.goal_x, e.goal_y, eef=(e.eef_x, e.eef_y))\n",
    "plt.subplot(122)\n",
    "plot_q(critic, e.eef_x, e.eef_y, e.circle.x, e.circle.y, e.goal_x, e.goal_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "batch_size = 1024\n",
    "replay_buffer = PriorityBuffer(2 ** 21)\n",
    "gamma = 0.98\n",
    "epsilon = 0.1\n",
    "\n",
    "X = np.zeros((batch_size, 6))\n",
    "Xp = np.zeros((batch_size, 6))\n",
    "U = np.zeros((batch_size, 2))\n",
    "R = np.zeros((batch_size, 1))\n",
    "gradient_len = actor.gradients(X[:1, :]).shape[1]\n",
    "policy_gradient = np.zeros((1, gradient_len))\n",
    "\n",
    "n_iterations = 2048.0\n",
    "latest_plot = datetime.now() - timedelta(seconds=30)\n",
    "latest_trial_plot = datetime.now() - timedelta(seconds=60)\n",
    "a = 0\n",
    "for a in range(a, int(n_iterations)):\n",
    "    print('iteration {} / {}'.format(a + 1, n_iterations))\n",
    "    e.reset()\n",
    "    latest_trial = []\n",
    "    latest_rewards = []\n",
    "    for b in range(batch_size):\n",
    "        x1 = e.get_state()\n",
    "        mu = actor.predict(x1)\n",
    "            \n",
    "        noise = np.random.randn(1, 2) * MAX_DIST * 1.0\n",
    "        mu = mu + noise\n",
    "        dist = np.linalg.norm(mu)\n",
    "        if dist > MAX_DIST:\n",
    "            mu = mu * MAX_DIST / dist\n",
    "        state, reward, x2 = e.interact(*(mu)[0, :])\n",
    "        latest_trial.append(x2[0, :])\n",
    "        latest_rewards.append(reward)\n",
    "        replay_buffer.add({\n",
    "            'x1': x1,\n",
    "            'x2': x2,\n",
    "            'u': mu,\n",
    "            'r': reward\n",
    "        }).set_value(10.0)\n",
    "        if state in [LOSE, WIN] or b == batch_size - 1:\n",
    "            if datetime.now() > latest_trial_plot + timedelta(seconds=60):\n",
    "                latest_trial_plot = datetime.now()\n",
    "                x = np.array(latest_trial)\n",
    "                plt.figure(figsize=(12, 3))\n",
    "                plt.subplot(121)\n",
    "                plt.plot(x[:, 0], x[:, 1], 'b')\n",
    "                plt.plot(x[:, 2], x[:, 3], 'ro', markersize=14, alpha=0.2)\n",
    "                plt.plot(x[:, 4], x[:, 5], 'ko', markersize=14)\n",
    "                plt.plot(x[:, 4], x[:, 5], 'wo', markersize=12)\n",
    "                plt.ylim((0.10, 0.30))\n",
    "                plt.xlim((-0.15, 0.15))\n",
    "                plt.subplot(122)\n",
    "                plt.plot(latest_rewards)\n",
    "                plt.show()\n",
    "            latest_trial = []\n",
    "            latest_rewards = []\n",
    "            e.reset()\n",
    "    \n",
    "    n_inner = 4\n",
    "    for i in range(n_inner):\n",
    "        exp_nodes = []\n",
    "        for b in range(batch_size):\n",
    "            sample = replay_buffer.sample()\n",
    "            exp_nodes.append(sample)\n",
    "            X[b, :] = sample.data['x1']\n",
    "            Xp[b, :] = sample.data['x2']\n",
    "            R[b, :] = sample.data['r']\n",
    "            U[b, :] = sample.data['u']\n",
    "        Q = critic.predict(X, U)\n",
    "        Y = R + gamma * critic_target.predict(Xp, actor_target.predict(Xp))\n",
    "        [node.set_value(abs(delta) + epsilon) for node, delta in zip(exp_nodes, (Q - Y)[:, 0])]\n",
    "        beta = np.exp((a - n_iterations) / (0.1 * n_iterations))\n",
    "        sample_weight = np.array([1.0 / node.value for node in exp_nodes]) ** beta\n",
    "        critic.nn.fit([X, U], Y, verbose=0, sample_weight=sample_weight)\n",
    "        policy_gradient *= 0\n",
    "        for b in range(batch_size):\n",
    "            policy_gradient += sample_weight[b] * np.dot(\n",
    "                critic.gradients(X[b:b + 1, :], U[b:b + 1, :]),\n",
    "                actor.gradients(X[b:b + 1, :])\n",
    "            ) / batch_size\n",
    "        actor.update_with_policy_gradient(policy_gradient, lr=0.001)\n",
    "        actor_target.soft_update(actor.nn.weights, lr=0.001)\n",
    "        critic_target.soft_update(critic.nn.weights, lr=0.001)\n",
    "        \n",
    "        if datetime.now() > latest_plot + timedelta(seconds=60):\n",
    "            print('beta: {} outer: {}/{} inner: {}/{} {}'.format(beta, a, n_iterations, i, n_inner, replay_buffer))\n",
    "            plt.figure(figsize=(13, 7))\n",
    "            plt.subplot(221)\n",
    "            e.circle.x = 0.00; e.circle.y = 0.20\n",
    "            e.eef_x = -0.04; e.eef_y = 0.20\n",
    "            e.goal_x = 0.04; e.goal_y = 0.20\n",
    "            plot_pi(actor, e.circle.x, e.circle.y, e.goal_x, e.goal_y, eef=(e.eef_x, e.eef_y))\n",
    "            plt.subplot(222)\n",
    "            plot_q(critic, e.eef_x, e.eef_y, e.circle.x, e.circle.y, e.goal_x, e.goal_y)\n",
    "            plt.subplot(223)\n",
    "            plot_pi(actor_target, e.circle.x, e.circle.y, e.goal_x, e.goal_y, eef=(e.eef_x, e.eef_y))\n",
    "            plt.subplot(224)\n",
    "            plot_q(critic_target, e.eef_x, e.eef_y, e.circle.x, e.circle.y, e.goal_x, e.goal_y)\n",
    "            plt.show()\n",
    "            latest_plot = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e.reset()\n",
    "e.eef_x = 0.05\n",
    "e.eef_y = 0.20\n",
    "e.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots()\n",
    "#for i in range(100):\n",
    "#    mu = actor.predict(e.get_state())\n",
    "#    s, _, _ = e.interact(*mu.flatten())\n",
    "#    e.plot(ax=ax)\n",
    "#    #plt.savefig('res/move{:04d}.png'.format(i))\n",
    "#    if s == LOSE:\n",
    "#        break\n",
    "\n",
    "def plot(e, ax, nn):\n",
    "    plt.grid()\n",
    "    ax.add_artist(plt.Circle(\n",
    "        (e.goal_x, e.goal_y),\n",
    "        e.circle.radius,\n",
    "        color='k',\n",
    "    ))\n",
    "    ax.add_artist(plt.Circle(\n",
    "        (e.goal_x, e.goal_y),\n",
    "        e.circle.radius - 0.001,\n",
    "        color='w',\n",
    "    ))\n",
    "    n_steps = 512\n",
    "    for i in range(n_steps):\n",
    "        if i % 2:\n",
    "            continue\n",
    "        mu = nn.predict(e.get_state())\n",
    "        s, _, _ = e.interact(*mu.flatten())\n",
    "        ax.add_artist(plt.Circle(\n",
    "            (e.circle.x, e.circle.y),\n",
    "            e.circle.radius,\n",
    "            color='r',\n",
    "            alpha=(1.0 * i / n_steps)\n",
    "        ))\n",
    "        plt.plot(e.eef_x, e.eef_y, 'k+', markersize=10, alpha=np.sqrt(1.0 * i / n_steps))\n",
    "    ax.add_artist(plt.Circle(\n",
    "        (e.circle.x, e.circle.y),\n",
    "        e.circle.radius,\n",
    "        color='k',\n",
    "    ))\n",
    "    ax.add_artist(plt.Circle(\n",
    "        (e.circle.x, e.circle.y),\n",
    "        e.circle.radius * 0.95,\n",
    "        color='r',\n",
    "    ))\n",
    "    plt.xlim((-0.15, 0.15))\n",
    "    plt.ylim((0.10, 0.30))\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "e.reset()\n",
    "plot(e, ax, actor)\n",
    "#plt.savefig('naf_sim_failure_mode_ideal.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
